{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BKJRKZNl6--"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    \"i love to eat pizza\",\n",
        "    \"i love to eat pasta\",\n",
        "    \"i love to write code\",\n",
        "    \"i learn rnn\",\n",
        "    \"rnn is simple\",\n",
        "    \"rnn is cool\",\n",
        "    \"i like programming\",\n",
        "    \"programming is fun\"\n",
        "]"
      ],
      "metadata": {
        "id": "opGYPkvXmPki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Preprocessing data...\")\n",
        "# Tokenize (split) sentences into words and build a vocabulary\n",
        "words = set()\n",
        "for sentence in corpus:\n",
        "    words.update(sentence.lower().split())\n",
        "\n",
        "vocab = sorted(list(words))\n",
        "vocab_size = len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kiHPQwnmGLS",
        "outputId": "b99b0ae3-f907-43ec-8853-01434aead8b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create mappings from words to numerical indices and back\n",
        "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
        "idx_to_word = {i: word for i, word in enumerate(vocab)}"
      ],
      "metadata": {
        "id": "htPekXOYmJ04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = []\n",
        "for sentence in corpus:\n",
        "    tokens = sentence.lower().split()\n",
        "    for i in range(1, len(tokens)):\n",
        "        context = tokens[:i]\n",
        "        target = tokens[i]\n",
        "        sequences.append((context, target))"
      ],
      "metadata": {
        "id": "VM-QIknTmWsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "print(f\"Number of training sequences: {len(sequences)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EnK5QpymXDU",
        "outputId": "8ed12790-af28-476e-9b7e-1a6a0a0fc72d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 16\n",
            "Number of training sequences: 22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class WordRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
        "        super(WordRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # 1. Embedding Layer: Turns word indices into dense vectors\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "\n",
        "        # 2. RNN Layer: Processes the sequence of embedded vectors\n",
        "        #    batch_first=True means input shape is (batch, seq_len, features)\n",
        "        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)\n",
        "\n",
        "        # 3. Fully Connected Layer: Maps RNN output to a prediction over the vocab\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x starts as: (batch_size, seq_len) [indices]\n",
        "\n",
        "        # 1. Pass through embedding layer\n",
        "        x = self.embedding(x)\n",
        "        # x is now: (batch_size, seq_len, embed_size) [vectors]\n",
        "\n",
        "        # 2. Pass through RNN\n",
        "        # We don't provide an initial hidden state, so nn.RNN defaults to zero\n",
        "        out, _ = self.rnn(x)\n",
        "        # out is: (batch_size, seq_len, hidden_size)\n",
        "\n",
        "        # 3. We only care about the output from the *last* time step\n",
        "        out = out[:, -1, :]\n",
        "        # out is now: (batch_size, hidden_size)\n",
        "\n",
        "        # 4. Pass through the final linear layer\n",
        "        out = self.fc(out)\n",
        "        # out is now: (batch_size, vocab_size) [logits for next word]\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "yDFCfX1_mYbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(self, x):\n",
        "        # x starts as: (batch_size, seq_len) [indices]\n",
        "\n",
        "        # 1. Pass through embedding layer\n",
        "        x = self.embedding(x)\n",
        "        # x is now: (batch_size, seq_len, embed_size) [vectors]\n",
        "\n",
        "        # 2. Pass through RNN\n",
        "        # We don't provide an initial hidden state, so nn.RNN defaults to zero\n",
        "        out, _ = self.rnn(x)\n",
        "        # out is: (batch_size, seq_len, hidden_size)\n",
        "\n",
        "        # 3. We only care about the output from the *last* time step\n",
        "        out = out[:, -1, :]\n",
        "        # out is now: (batch_size, hidden_size)\n",
        "\n",
        "        # 4. Pass through the final linear layer\n",
        "        out = self.fc(out)\n",
        "        # out is now: (batch_size, vocab_size) [logits for next word]\n",
        "        return out"
      ],
      "metadata": {
        "id": "D3S-DTYSmbBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------\n",
        "\n",
        "# Hyperparameters\n",
        "EMBED_SIZE = 10\n",
        "HIDDEN_SIZE = 32\n",
        "LEARNING_RATE = 0.01\n",
        "EPOCHS = 200\n",
        "\n",
        "# Instantiate model, loss, and optimizer\n",
        "model = WordRNN(vocab_size, EMBED_SIZE, HIDDEN_SIZE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "print(\"Starting training...\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    total_loss = 0\n",
        "\n",
        "    # We train one sequence at a time (batch size = 1) for simplicity\n",
        "    for context, target in sequences:\n",
        "        # --- Prepare Tensors ---\n",
        "        # Convert context words to indices\n",
        "        context_idxs = [word_to_idx[w] for w in context]\n",
        "        context_tensor = torch.tensor(context_idxs, dtype=torch.long).unsqueeze(0) # (1, seq_len)\n",
        "\n",
        "        # Convert target word to index\n",
        "        target_idx = [word_to_idx[target]]\n",
        "        target_tensor = torch.tensor(target_idx, dtype=torch.long) # (1)\n",
        "\n",
        "        # --- Forward Pass ---\n",
        "        optimizer.zero_grad()\n",
        "        output = model(context_tensor) # (1, vocab_size)\n",
        "\n",
        "        # --- Calculate Loss & Backpropagate ---\n",
        "        loss = criterion(output, target_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {total_loss/len(sequences):.4f}\")\n",
        "\n",
        "print(\"Training finished.\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 5. Inference (Autocomplete Function)\n",
        "# ----------------------------------------\n",
        "\n",
        "def autocomplete(model, seed_text, n_words=3):\n",
        "    \"\"\"\n",
        "    Generates n_words following the seed_text.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    # Use torch.no_grad() to disable gradient calculations\n",
        "    with torch.no_grad():\n",
        "        current_text = seed_text.lower()\n",
        "        generated_words = current_text.split()\n",
        "\n",
        "        for _ in range(n_words):\n",
        "            # Prepare the input tensor\n",
        "            try:\n",
        "                context_idxs = [word_to_idx[w] for w in generated_words]\n",
        "            except KeyError as e:\n",
        "                print(f\"Error: Word '{e.args[0]}' not in vocabulary.\")\n",
        "                return ' '.join(generated_words)\n",
        "\n",
        "            context_tensor = torch.tensor(context_idxs, dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "            # Get the model's prediction\n",
        "            output = model(context_tensor) # (1, vocab_size)\n",
        "\n",
        "            # Get the index of the most likely next word\n",
        "            pred_idx = torch.argmax(output, dim=1).item()\n",
        "\n",
        "            # Convert index back to a word\n",
        "            next_word = idx_to_word[pred_idx]\n",
        "\n",
        "            # Append the new word to our sequence\n",
        "            generated_words.append(next_word)\n",
        "\n",
        "    return ' '.join(generated_words)\n",
        "\n",
        "\n",
        "print(\"\\n--- Autocomplete Examples ---\")\n",
        "\n",
        "seed1 = \"i love\"\n",
        "print(f\"Seed: '{seed1}' -> Prediction: '{autocomplete(model, seed1, n_words=2)}'\")\n",
        "\n",
        "seed2 = \"rnn is\"\n",
        "print(f\"Seed: '{seed2}' -> Prediction: '{autocomplete(model, seed2, n_words=2)}'\")\n",
        "\n",
        "seed3 = \"i learn\"\n",
        "print(f\"Seed: '{seed3}' -> Prediction: '{autocomplete(model, seed3, n_words=1)}'\")\n",
        "\n",
        "seed4 = \"programming is\"\n",
        "print(f\"Seed: '{seed4}' -> Prediction: '{autocomplete(model, seed4, n_words=1)}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "malryMwfmcqk",
        "outputId": "7f08e7f2-b6ed-417d-9f63-51e817ef5bd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch [20/200], Loss: 0.5291\n",
            "Epoch [40/200], Loss: 0.5110\n",
            "Epoch [60/200], Loss: 0.5023\n",
            "Epoch [80/200], Loss: 0.4990\n",
            "Epoch [100/200], Loss: 0.4968\n",
            "Epoch [120/200], Loss: 0.4946\n",
            "Epoch [140/200], Loss: 0.4890\n",
            "Epoch [160/200], Loss: 0.4891\n",
            "Epoch [180/200], Loss: 0.4858\n",
            "Epoch [200/200], Loss: 0.4781\n",
            "Training finished.\n",
            "\n",
            "--- Autocomplete Examples ---\n",
            "Seed: 'i love' -> Prediction: 'i love to eat'\n",
            "Seed: 'rnn is' -> Prediction: 'rnn is simple is'\n",
            "Seed: 'i learn' -> Prediction: 'i learn rnn'\n",
            "Seed: 'programming is' -> Prediction: 'programming is fun'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mIAUbzTAmfg7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}